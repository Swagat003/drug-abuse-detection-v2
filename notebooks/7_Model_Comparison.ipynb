{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae9c1d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98f9ffe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BERT_bert-base_metrics.csv',\n",
       " 'BERT_RoBERTa-base_metrics.csv',\n",
       " 'BERT_SciBERT-base_metrics.csv',\n",
       " 'Linear_SVM_metrics.csv',\n",
       " 'Logistic_Regression_metrics.csv']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "METRICS_PATH = \"../Models/Evaluation/metrics\"\n",
    "\n",
    "files = os.listdir(METRICS_PATH)\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b56c626",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bert-base</td>\n",
       "      <td>0.992984</td>\n",
       "      <td>0.994647</td>\n",
       "      <td>0.991330</td>\n",
       "      <td>0.992986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RoBERTa-base</td>\n",
       "      <td>0.992984</td>\n",
       "      <td>0.994647</td>\n",
       "      <td>0.991330</td>\n",
       "      <td>0.992986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SciBERT-base</td>\n",
       "      <td>0.992817</td>\n",
       "      <td>0.990704</td>\n",
       "      <td>0.994998</td>\n",
       "      <td>0.992846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Linear_SVM</td>\n",
       "      <td>0.985968</td>\n",
       "      <td>0.992150</td>\n",
       "      <td>0.979745</td>\n",
       "      <td>0.985908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Logistic_Regression</td>\n",
       "      <td>0.985342</td>\n",
       "      <td>0.991144</td>\n",
       "      <td>0.979495</td>\n",
       "      <td>0.985285</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 model  accuracy  precision    recall  f1_score\n",
       "0            bert-base  0.992984   0.994647  0.991330  0.992986\n",
       "1         RoBERTa-base  0.992984   0.994647  0.991330  0.992986\n",
       "2         SciBERT-base  0.992817   0.990704  0.994998  0.992846\n",
       "3           Linear_SVM  0.985968   0.992150  0.979745  0.985908\n",
       "4  Logistic_Regression  0.985342   0.991144  0.979495  0.985285"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_metrics = []\n",
    "\n",
    "for file in files:\n",
    "    if file.endswith(\"_metrics.csv\"):\n",
    "        df = pd.read_csv(os.path.join(METRICS_PATH, file))\n",
    "        all_metrics.append(df)\n",
    "\n",
    "comparison_df = pd.concat(all_metrics, ignore_index=True)\n",
    "comparison_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e67b7575",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df.to_csv(\n",
    "    \"../Models/Evaluation/final_model_comparison.csv\",\n",
    "    index=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2d9fb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(\n",
    "    data=comparison_df,\n",
    "    x=\"model\",\n",
    "    y=\"accuracy\"\n",
    ")\n",
    "\n",
    "plt.ylim(0.95, 1.0)\n",
    "plt.xticks(rotation=30)\n",
    "plt.title(\"Model Accuracy Comparison\")\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\n",
    "    \"../Models/Evaluation/plots/model_accuracy_comparison.png\",\n",
    "    dpi=300\n",
    ")\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef60429d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(\n",
    "    data=comparison_df,\n",
    "    x=\"model\",\n",
    "    y=\"recall\"\n",
    ")\n",
    "\n",
    "plt.ylim(0.95, 1.0)\n",
    "plt.xticks(rotation=30)\n",
    "plt.title(\"Model Recall Comparison (Drug Abuse Detection)\")\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\n",
    "    \"../Models/Evaluation/plots/model_recall_comparison.png\",\n",
    "    dpi=300\n",
    ")\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d7114bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(\n",
    "    data=comparison_df,\n",
    "    x=\"model\",\n",
    "    y=\"f1_score\"\n",
    ")\n",
    "\n",
    "plt.ylim(0.95, 1.0)\n",
    "plt.xticks(rotation=30)\n",
    "plt.title(\"Model F1-Score Comparison\")\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\n",
    "    \"../Models/Evaluation/plots/model_f1_comparison.png\",\n",
    "    dpi=300\n",
    ")\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "38045fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\"accuracy\", \"precision\", \"recall\", \"f1_score\"]\n",
    "angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False).tolist()\n",
    "angles += angles[:1]\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "ax = plt.subplot(111, polar=True)\n",
    "\n",
    "for _, row in comparison_df.iterrows():\n",
    "    values = row[metrics].tolist()\n",
    "    values += values[:1]\n",
    "    ax.plot(angles, values, label=row[\"model\"])\n",
    "    ax.fill(angles, values, alpha=0.1)\n",
    "\n",
    "ax.set_thetagrids(np.degrees(angles[:-1]), metrics)\n",
    "ax.set_title(\"Radar Chart: Model Performance Comparison\")\n",
    "ax.set_ylim(0.97, 1.0)\n",
    "ax.legend(loc=\"upper right\", bbox_to_anchor=(1.0, 0.15))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\n",
    "    \"../Models/Evaluation/plots/radar_model_comparison.png\",\n",
    "    dpi=300\n",
    ")\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "39e13b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "for metric in [\"accuracy\", \"precision\", \"recall\", \"f1_score\"]:\n",
    "    plt.plot(\n",
    "        comparison_df[\"model\"],\n",
    "        comparison_df[metric],\n",
    "        marker=\"o\",\n",
    "        label=metric\n",
    "    )\n",
    "\n",
    "plt.ylim(0.97, 1.0)\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Metric-wise Comparison Across Models\")\n",
    "plt.xticks(rotation=30)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\n",
    "    \"../Models/Evaluation/plots/line_metric_comparison.png\",\n",
    "    dpi=300\n",
    ")\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a7d1d680",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "heatmap_df = comparison_df.set_index(\"model\")\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "sns.heatmap(\n",
    "    heatmap_df,\n",
    "    annot=True,\n",
    "    fmt=\".3f\",\n",
    "    cmap=\"YlGnBu\"\n",
    ")\n",
    "\n",
    "plt.title(\"Heatmap of Model Performance Metrics\")\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\n",
    "    \"../Models/Evaluation/plots/heatmap_model_comparison.png\",\n",
    "    dpi=300\n",
    ")\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c51d0fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
